{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Title\n",
    "\n",
    "This notebook lists all the steps that you need to complete the complete this project. You will need to complete all the TODOs in this notebook as well as in the README and the two python scripts included with the starter code.\n",
    "\n",
    "\n",
    "**TODO**: Give a helpful introduction to what this notebook is for. Remember that comments, explanations and good documentation make your project informative and professional.\n",
    "\n",
    "**Note:** This notebook has a bunch of code and markdown cells with TODOs that you have to complete. These are meant to be helpful guidelines for you to finish your project while meeting the requirements in the project rubrics. Feel free to change the order of these the TODO's and use more than one TODO code cell to do all your tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install packages for Debugging and Profiling on SageMaker\n",
    "# 'smdebug' may have error with latest version, so we should use another version.\n",
    "## Reference: https://pypi.org/project/smdebug/#history\n",
    "!pip install -U smdebug==1.0.11 sagemaker torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\tranh\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Import any packages that you might need\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner,\n",
    ")\n",
    "from sagemaker.debugger import (\n",
    "    Rule, ProfilerRule, rule_configs,\n",
    "    DebuggerHookConfig, ProfilerConfig, FrameworkProfile\n",
    ")\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Settings\n",
    "role = \"arn:aws:iam::008748484958:role/project03-khangtictoc\"\n",
    "region = \"us-east-1\"\n",
    "\n",
    "bucket = \"project03-khangtictoc\"\n",
    "prefix = \"dataset\"\n",
    "\n",
    "debugger_s3_output = \"s3://{}/debugger-output\".format(bucket)\n",
    "profiler_s3_output = \"s3://{}/profiler-output\".format(bucket)\n",
    "local_dataset_path = \"./dataset/dogImages\"\n",
    "\n",
    "image_name = \"resnet50-training-job\"\n",
    "ecr_name = \"public.ecr.aws/q1p8o7w7/resnet50-training-job:latest\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create channel for data input's location\n",
    "train_loc = \"s3://project03-khangtictoc/dataset/train\"\n",
    "validation_loc = \"s3://project03-khangtictoc/dataset/valid\"\n",
    "test_loc = \"s3://project03-khangtictoc/dataset/test\"\n",
    "\n",
    "channels = {\n",
    "    \"training\": train_loc,\n",
    "    \"validation\": validation_loc,\n",
    "    \"testing\": test_loc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure credentials\n",
    "!aws configure set aws_access_key_id AKIAQECLTJFPC7HYHHWP\n",
    "!aws configure set aws_secret_access_key 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\n",
    "!aws configure set default.region us-east-1\n",
    "#!aws configure set aws_session_token IQoJb3JpZ2luX2VjELf//////////wEaCXVzLXdlc3QtMiJHMEUCICttpucjF6b3A0HBRw2125tFRNweQOHGt7s+p4H+V5NvAiEAmgPbsr5RILM3NYjzUpBHmmfiVEY8I2G53ojn1DYO7w4qvgIIMBADGgw1OTA3MTQzMDk0NTQiDP5I8ztyCeXOT7vgICqbArqFFH9OfhTL6DHjNtixWB37TyVO6ropbfsQDxgGtsH4R+9bIsNV0wV3mjnmul3JEx9NS69WfazRlxBVfq/G6N3vZGtAAPVLn3uHmo59FOdo2YfkouI42j+0u7Y/3K8wj1wxPTX1VQTJ29mrfymJqtartiGRqca/DbWSILfcsRrNQnCsLFIAJ+UoRAaPMG3vqjywwJGruqcpsypjVdze3NSyKsjvLWUFr5j3pkkuHk+5enVqQ2sN/33L7ZM+jv5BfmMGoU/Jym4NMYPsq4rqJ1i5KuY1B9i1dsanGbnIbiFNHj/bAkPWec/GOgsLwcGQNRW3EoyQsTSF9BANsXIoybzDSZmfuhGP6rb5vYS8mepXkrj+0ZRBhjL5huowwMetsgY6nQE6Uz91LVGZ2W7i/ELkyxcZfoH3PbdnICROnrkf+hN3MI63Za2WFY6ZiXBW8tDbgcnhaPA2bwQzfOgbrjm35dh/bBS2zctzK+KIOfGyRFVto0je05hMjf0iT18yIfsC0tpINTIIfRcdDsQHWO0dcwmon9vUnUxwTDKO7dzqvgdQvtQWYgIfqFwNwbFj5aoDiQQhEqNB5TiJUF82Uc8i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "TODO: Explain what dataset you are using for this project. Maybe even give a small overview of the classes, class distributions etc that can help anyone not familiar with the dataset get a better understand of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and sync to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and upload the data to AWS S3\n",
    "\n",
    "# Command to download and unzip data\n",
    "!wget https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n",
    "!unzip dogImages.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DEFAULT_S3_BUCKET\"] = bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws s3api create-bucket --bucket %DEFAULT_S3_BUCKET%\n",
    "\n",
    "# Window\n",
    "!aws s3 sync ./dataset/dogImages/train/ s3://%DEFAULT_S3_BUCKET%/dataset/train/\n",
    "!aws s3 sync ./dataset/dogImages/test/ s3://%DEFAULT_S3_BUCKET%/dataset/test/\n",
    "!aws s3 sync ./dataset/dogImages/valid/ s3://%DEFAULT_S3_BUCKET%/dataset/valid/\n",
    "\n",
    "# Linux\n",
    "#!aws s3 sync ./dataset/train s3://${DEFAULT_S3_BUCKET}/train/\n",
    "#!aws s3 sync ./dataset/test s3://${DEFAULT_S3_BUCKET}/test/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader (Optional)\n",
    "\n",
    "Use local workspace for investigating data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define transforming actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.ImageFolder(\"./dataset/dogImages/train\", transform=transforms)\n",
    "valset = datasets.ImageFolder(\"./dataset/dogImages/valid\", transform=transforms)\n",
    "testset = datasets.ImageFolder(\"./dataset/dogImages/test\", transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['001.Affenpinscher',\n",
       " '002.Afghan_hound',\n",
       " '003.Airedale_terrier',\n",
       " '004.Akita',\n",
       " '005.Alaskan_malamute']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show labels\n",
    "train_loader.dataset.classes[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('./dataset/dogImages/train\\\\129.Tibetan_mastiff\\\\Tibetan_mastiff_08184.jpg',\n",
       "  128),\n",
       " ('./dataset/dogImages/train\\\\025.Black_and_tan_coonhound\\\\Black_and_tan_coonhound_01781.jpg',\n",
       "  24),\n",
       " ('./dataset/dogImages/train\\\\091.Japanese_chin\\\\Japanese_chin_06201.jpg', 90),\n",
       " ('./dataset/dogImages/train\\\\104.Miniature_schnauzer\\\\Miniature_schnauzer_06887.jpg',\n",
       "  103),\n",
       " ('./dataset/dogImages/train\\\\094.Komondor\\\\Komondor_06353.jpg', 93)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View some samples and labels\n",
    "random_img = random.sample(train_loader.dataset.imgs, 5)\n",
    "random_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File extensions\n",
    "train_loader.dataset.extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 133\n",
      "Number of samples: 6680\n"
     ]
    }
   ],
   "source": [
    "# Number of labels\n",
    "print(\"Number of labels: %d\" % len(train_loader.dataset.classes))\n",
    "\n",
    "# Number of samples\n",
    "print(\"Number of samples: %d\" % len(train_loader.dataset.imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "TODO: This is the part where you will finetune a pretrained model with hyperparameter tuning. Remember that you have to tune a minimum of two hyperparameters. However you are encouraged to tune more. You are also encouraged to explain why you chose to tune those particular hyperparameters and the ranges.\n",
    "\n",
    "Note: You will need to use the hpo.py script to perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare HP ranges, metrics etc.\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(0.001, 0.1),\n",
    "    \"batch-size\": CategoricalParameter([16, 32, 64, 128, 256, 512]),\n",
    "    \"epochs\": IntegerParameter(10, 20)\n",
    "}\n",
    "\n",
    "objective_metric_name = \"average test loss\"\n",
    "objective_type = \"Minimize\"\n",
    "metric_definitions = [{\"Name\": \"average test loss\", \"Regex\": \"Test set: Average loss: ([0-9\\\\.]+)\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimators for HPs\n",
    "\n",
    "estimator = PyTorch(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.4xlarge\",\n",
    "    source_dir=\"scripts\",\n",
    "    entry_point=\"hpo.py\",\n",
    "    framework_version=\"2.2\",\n",
    "    py_version=\"py310\",\n",
    ")\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions,\n",
    "    max_jobs=4,\n",
    "    max_parallel_jobs=1,\n",
    "    objective_type=objective_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "# Fit HP Tuner\n",
    "tuner.fit(inputs=channels, wait=True) # Include data channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-05-25 02:25:35 Starting - Preparing the instances for training\n",
      "2024-05-25 02:25:35 Downloading - Downloading the training image\n",
      "2024-05-25 02:25:35 Training - Training image download completed. Training in progress.\n",
      "2024-05-25 02:25:35 Uploading - Uploading generated training model\n",
      "2024-05-25 02:25:35 Completed - Resource reused by training job: pytorch-training-240525-0913-002-d7b62d3b\n"
     ]
    }
   ],
   "source": [
    "# Get the best estimators and the best HPs\n",
    "best_estimator = tuner.best_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_tuning_objective_metric': '\"average test loss\"',\n",
       " 'batch-size': '\"64\"',\n",
       " 'epochs': '17',\n",
       " 'lr': '0.003150698302831685',\n",
       " 'sagemaker_container_log_level': '20',\n",
       " 'sagemaker_estimator_class_name': '\"PyTorch\"',\n",
       " 'sagemaker_estimator_module': '\"sagemaker.pytorch.estimator\"',\n",
       " 'sagemaker_job_name': '\"pytorch-training-2024-05-25-02-12-57-266\"',\n",
       " 'sagemaker_program': '\"hpo.py\"',\n",
       " 'sagemaker_region': '\"us-east-1\"',\n",
       " 'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-008748484958/pytorch-training-2024-05-25-02-12-57-266/source/sourcedir.tar.gz\"'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the hyperparameters of the best trained model\n",
    "best_estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': '0.003150698302831685', 'batch-size': 64, 'epochs': '17'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create hyperparameter dict for the best model for later use\n",
    "best_hyperparameters = {\n",
    "    \"lr\": best_estimator.hyperparameters()['lr'],\n",
    "    \"batch-size\": int(best_estimator.hyperparameters()['batch-size'].replace('\"', '')),\n",
    "    \"epochs\": best_estimator.hyperparameters()['epochs']\n",
    "}\n",
    "best_hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Profiling and Debugging\n",
    "TODO: Using the best hyperparameters, create and finetune a new model\n",
    "\n",
    "Note: You will need to use the train_model.py script to perform model profiling and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Debugger\n",
    "\n",
    "debugger_hook_config = DebuggerHookConfig(\n",
    "    s3_output_path=debugger_s3_output,\n",
    "    container_local_output_path=debugger_s3_output,\n",
    "    hook_parameters={\n",
    "        \"train.save_interval\": \"100\",\n",
    "        \"eval.save_interval\": \"10\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework profiling will be deprecated from tensorflow 2.12 and pytorch 2.0 in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "# Configure Profiler\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500,\n",
    "    framework_profile_params=FrameworkProfile(num_steps=10),\n",
    "    s3_output_path=profiler_s3_output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the rules for debugging\n",
    "\n",
    "rules = [\n",
    "    Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
    "    ProfilerRule.sagemaker(rule_configs.LowGPUUtilization()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "    Rule.sagemaker(rule_configs.vanishing_gradient()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    Rule.sagemaker(rule_configs.overtraining()),\n",
    "    Rule.sagemaker(rule_configs.poor_weight_initialization()),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'008748484958.dkr.ecr.us-east-1.amazonaws.com/resnet50-training-job:latest'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client=boto3.client('sts')\n",
    "account=client.get_caller_identity()['Account']\n",
    "\n",
    "my_session=boto3.session.Session()\n",
    "region=my_session.region_name\n",
    "\n",
    "ecr_image='{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, image_name)\n",
    "\n",
    "ecr_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: concac-2024-05-26-06-18-34-721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26 06:18:41 Starting - Starting the training job...\n",
      "2024-05-26 06:18:58 Starting - Preparing the instances for training...\n",
      "2024-05-26 06:19:23 Downloading - Downloading input data...\n",
      "2024-05-26 06:20:03 Downloading - Downloading the training image........................\n",
      "2024-05-26 06:24:31 Failed - Training job failed\n",
      ".."
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job concac-2024-05-26-06-18-34-721: Failed. Reason: InternalServerError: We encountered an internal error. Please try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 26\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create and fit an estimator\u001b[39;00m\n\u001b[0;32m      3\u001b[0m estimator \u001b[38;5;241m=\u001b[39m PyTorch(\n\u001b[0;32m      4\u001b[0m     image_uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcac\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# profiler_config=profiler_config\u001b[39;00m\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 26\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tranh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\workflow\\pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tranh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\estimator.py:1346\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[1;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[0;32m   1345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m-> 1346\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_training_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tranh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\estimator.py:2703\u001b[0m, in \u001b[0;36m_TrainingJob.wait\u001b[1;34m(self, logs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[38;5;66;03m# If logs are requested, call logs_for_jobs.\u001b[39;00m\n\u001b[0;32m   2702\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 2703\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2704\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2705\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mwait_for_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name)\n",
      "File \u001b[1;32mc:\\Users\\tranh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\session.py:5797\u001b[0m, in \u001b[0;36mSession.logs_for_job\u001b[1;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[0;32m   5776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlogs_for_job\u001b[39m(\u001b[38;5;28mself\u001b[39m, job_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, poll\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, log_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll\u001b[39m\u001b[38;5;124m\"\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   5777\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Display logs for a given training job, optionally tailing them until job is complete.\u001b[39;00m\n\u001b[0;32m   5778\u001b[0m \n\u001b[0;32m   5779\u001b[0m \u001b[38;5;124;03m    If the output is a tty or a Jupyter cell, it will be color-coded\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5795\u001b[0m \u001b[38;5;124;03m        exceptions.UnexpectedStatusException: If waiting and the training job fails.\u001b[39;00m\n\u001b[0;32m   5796\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5797\u001b[0m     \u001b[43m_logs_for_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tranh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\session.py:8026\u001b[0m, in \u001b[0;36m_logs_for_job\u001b[1;34m(sagemaker_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[0;32m   8023\u001b[0m             last_profiler_rule_statuses \u001b[38;5;241m=\u001b[39m profiler_rule_statuses\n\u001b[0;32m   8025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m-> 8026\u001b[0m     \u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrainingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   8027\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dot:\n\u001b[0;32m   8028\u001b[0m         \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\tranh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sagemaker\\session.py:8079\u001b[0m, in \u001b[0;36m_check_job_status\u001b[1;34m(job, desc, status_key_name)\u001b[0m\n\u001b[0;32m   8073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[0;32m   8074\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[0;32m   8075\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[0;32m   8076\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   8077\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[0;32m   8078\u001b[0m     )\n\u001b[1;32m-> 8079\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[0;32m   8080\u001b[0m     message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[0;32m   8081\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   8082\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[0;32m   8083\u001b[0m )\n",
      "\u001b[1;31mUnexpectedStatusException\u001b[0m: Error for Training job concac-2024-05-26-06-18-34-721: Failed. Reason: InternalServerError: We encountered an internal error. Please try again."
     ]
    }
   ],
   "source": [
    "# Create and fit an estimator\n",
    "\n",
    "estimator = PyTorch(\n",
    "    image_uri=\"concac\",\n",
    "    role=role,\n",
    "\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.4xlarge\",\n",
    "\n",
    "    source_dir=\"scripts\",\n",
    "    entry_point=\"train_model.py\",\n",
    "    # framework_version=\"2.2\",\n",
    "    # py_version=\"py310\",\n",
    "    hyperparameters={'lr': '0.003150698302831685', 'batch-size': 64, 'epochs': '17'},\n",
    "\n",
    "    training_repository_access_mode='Vpc',\n",
    "    vpc_config=\"vpc-0393b7a0d43ba8bc7\",\n",
    "    subnets=[\"subnet-0c2f961dc23318518\"],\n",
    "    security_group_ids=['sg-0728a31b49f0e0c33'],\n",
    "\n",
    "    # rules=rules,\n",
    "    # debugger_hook_config=debugger_hook_config,\n",
    "    # profiler_config=profiler_config\n",
    ")\n",
    "\n",
    "estimator.fit(inputs=channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a debugging output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Is there some anomalous behaviour in your debugging output? If so, what is the error and how will you fix it?  \n",
    "**TODO**: If not, suppose there was an error. What would that error look like and how would you have fixed it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the profiler output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Analyze Profiling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Download Debugger Profiling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deploying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy your model to an endpoint\n",
    "\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Affenpinscher',\n",
       " 1: 'Afghan_hound',\n",
       " 2: 'Airedale_terrier',\n",
       " 3: 'Akita',\n",
       " 4: 'Alaskan_malamute',\n",
       " 5: 'American_eskimo_dog',\n",
       " 6: 'American_foxhound',\n",
       " 7: 'American_staffordshire_terrier',\n",
       " 8: 'American_water_spaniel',\n",
       " 9: 'Anatolian_shepherd_dog',\n",
       " 10: 'Australian_cattle_dog',\n",
       " 11: 'Australian_shepherd',\n",
       " 12: 'Australian_terrier',\n",
       " 13: 'Basenji',\n",
       " 14: 'Basset_hound',\n",
       " 15: 'Beagle',\n",
       " 16: 'Bearded_collie',\n",
       " 17: 'Beauceron',\n",
       " 18: 'Bedlington_terrier',\n",
       " 19: 'Belgian_malinois',\n",
       " 20: 'Belgian_sheepdog',\n",
       " 21: 'Belgian_tervuren',\n",
       " 22: 'Bernese_mountain_dog',\n",
       " 23: 'Bichon_frise',\n",
       " 24: 'Black_and_tan_coonhound',\n",
       " 25: 'Black_russian_terrier',\n",
       " 26: 'Bloodhound',\n",
       " 27: 'Bluetick_coonhound',\n",
       " 28: 'Border_collie',\n",
       " 29: 'Border_terrier',\n",
       " 30: 'Borzoi',\n",
       " 31: 'Boston_terrier',\n",
       " 32: 'Bouvier_des_flandres',\n",
       " 33: 'Boxer',\n",
       " 34: 'Boykin_spaniel',\n",
       " 35: 'Briard',\n",
       " 36: 'Brittany',\n",
       " 37: 'Brussels_griffon',\n",
       " 38: 'Bull_terrier',\n",
       " 39: 'Bulldog',\n",
       " 40: 'Bullmastiff',\n",
       " 41: 'Cairn_terrier',\n",
       " 42: 'Canaan_dog',\n",
       " 43: 'Cane_corso',\n",
       " 44: 'Cardigan_welsh_corgi',\n",
       " 45: 'Cavalier_king_charles_spaniel',\n",
       " 46: 'Chesapeake_bay_retriever',\n",
       " 47: 'Chihuahua',\n",
       " 48: 'Chinese_crested',\n",
       " 49: 'Chinese_shar-pei',\n",
       " 50: 'Chow_chow',\n",
       " 51: 'Clumber_spaniel',\n",
       " 52: 'Cocker_spaniel',\n",
       " 53: 'Collie',\n",
       " 54: 'Curly-coated_retriever',\n",
       " 55: 'Dachshund',\n",
       " 56: 'Dalmatian',\n",
       " 57: 'Dandie_dinmont_terrier',\n",
       " 58: 'Doberman_pinscher',\n",
       " 59: 'Dogue_de_bordeaux',\n",
       " 60: 'English_cocker_spaniel',\n",
       " 61: 'English_setter',\n",
       " 62: 'English_springer_spaniel',\n",
       " 63: 'English_toy_spaniel',\n",
       " 64: 'Entlebucher_mountain_dog',\n",
       " 65: 'Field_spaniel',\n",
       " 66: 'Finnish_spitz',\n",
       " 67: 'Flat-coated_retriever',\n",
       " 68: 'French_bulldog',\n",
       " 69: 'German_pinscher',\n",
       " 70: 'German_shepherd_dog',\n",
       " 71: 'German_shorthaired_pointer',\n",
       " 72: 'German_wirehaired_pointer',\n",
       " 73: 'Giant_schnauzer',\n",
       " 74: 'Glen_of_imaal_terrier',\n",
       " 75: 'Golden_retriever',\n",
       " 76: 'Gordon_setter',\n",
       " 77: 'Great_dane',\n",
       " 78: 'Great_pyrenees',\n",
       " 79: 'Greater_swiss_mountain_dog',\n",
       " 80: 'Greyhound',\n",
       " 81: 'Havanese',\n",
       " 82: 'Ibizan_hound',\n",
       " 83: 'Icelandic_sheepdog',\n",
       " 84: 'Irish_red_and_white_setter',\n",
       " 85: 'Irish_setter',\n",
       " 86: 'Irish_terrier',\n",
       " 87: 'Irish_water_spaniel',\n",
       " 88: 'Irish_wolfhound',\n",
       " 89: 'Italian_greyhound',\n",
       " 90: 'Japanese_chin',\n",
       " 91: 'Keeshond',\n",
       " 92: 'Kerry_blue_terrier',\n",
       " 93: 'Komondor',\n",
       " 94: 'Kuvasz',\n",
       " 95: 'Labrador_retriever',\n",
       " 96: 'Lakeland_terrier',\n",
       " 97: 'Leonberger',\n",
       " 98: 'Lhasa_apso',\n",
       " 99: 'Lowchen',\n",
       " 100: 'Maltese',\n",
       " 101: 'Manchester_terrier',\n",
       " 102: 'Mastiff',\n",
       " 103: 'Miniature_schnauzer',\n",
       " 104: 'Neapolitan_mastiff',\n",
       " 105: 'Newfoundland',\n",
       " 106: 'Norfolk_terrier',\n",
       " 107: 'Norwegian_buhund',\n",
       " 108: 'Norwegian_elkhound',\n",
       " 109: 'Norwegian_lundehund',\n",
       " 110: 'Norwich_terrier',\n",
       " 111: 'Nova_scotia_duck_tolling_retriever',\n",
       " 112: 'Old_english_sheepdog',\n",
       " 113: 'Otterhound',\n",
       " 114: 'Papillon',\n",
       " 115: 'Parson_russell_terrier',\n",
       " 116: 'Pekingese',\n",
       " 117: 'Pembroke_welsh_corgi',\n",
       " 118: 'Petit_basset_griffon_vendeen',\n",
       " 119: 'Pharaoh_hound',\n",
       " 120: 'Plott',\n",
       " 121: 'Pointer',\n",
       " 122: 'Pomeranian',\n",
       " 123: 'Poodle',\n",
       " 124: 'Portuguese_water_dog',\n",
       " 125: 'Saint_bernard',\n",
       " 126: 'Silky_terrier',\n",
       " 127: 'Smooth_fox_terrier',\n",
       " 128: 'Tibetan_mastiff',\n",
       " 129: 'Welsh_springer_spaniel',\n",
       " 130: 'Wirehaired_pointing_griffon',\n",
       " 131: 'Xoloitzcuintli',\n",
       " 132: 'Yorkshire_terrier'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dictionary for class-to-label mapping\n",
    "\n",
    "test_path = os.path.join(local_dataset_path, \"test\")\n",
    "\n",
    "label_breed_mapping = {k:v.split(\".\")[1] for k, v in enumerate(os.listdir(test_path))}\n",
    "label_breed_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run an prediction on the endpoint\n",
    "\n",
    "predictor.serializer = IdentitySerializer(\"image/png\")\n",
    "\n",
    "test_folder = \"012.Australian_shepherd\"\n",
    "test_image = \"Australian_shepherd_00830.jpg\"\n",
    "\n",
    "with open(os.path.join(test_path, test_folder, test_image), \"rb\") as f:\n",
    "    image = f.read()\n",
    "response = predictor.predict(image)\n",
    "\n",
    "print(\"Expected label: \\\"{}\\\" with  index of {}\".format(\n",
    "      test_folder.split(\".\")[1],\n",
    "      test_folder.split(\".\")[0]))\n",
    "print(\"Predicted label: \\\"{}\\\" with index of {}\".format(\n",
    "    response,\n",
    "    label_breed_mapping[response]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IIPORTANT) Shutdown/delete your endpoint once your work is done\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up remote registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecr_client = boto3.client(\"ecr\")\n",
    "ecr_response = ecr_client.create_repository(repositoryName=\"only-test\")\n",
    "ecr_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"repository\": {\n",
      "        \"repositoryArn\": \"arn:aws:ecr-public::008748484958:repository/resnet50-training-job\",\n",
      "        \"registryId\": \"008748484958\",\n",
      "        \"repositoryName\": \"resnet50-training-job\",\n",
      "        \"repositoryUri\": \"public.ecr.aws/q1p8o7w7/resnet50-training-job\",\n",
      "        \"createdAt\": \"2024-05-24T15:42:19.395000+07:00\"\n",
      "    },\n",
      "    \"catalogData\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Create ECR Repository if not exists\n",
    "!aws ecr-public create-repository --repository-name {image_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in C:\\Users\\tranh\\.docker\\config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Authenticate AWS ECR\n",
    "# Push command on Console\n",
    "#!aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com\n",
    "!aws ecr-public get-login-password --region us-east-1 | docker login --username AWS --password-stdin public.ecr.aws/q1p8o7w7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload image to registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package training model to Docker Image\n",
    "!docker build -t {image_name} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [public.ecr.aws/q1p8o7w7/resnet50-training-job]\n",
      "bbe00fe64b55: Preparing\n",
      "9501e9fcc404: Preparing\n",
      "1c4281c1b467: Preparing\n",
      "8f3dd49b5d19: Preparing\n",
      "555315c92a50: Preparing\n",
      "3be02ac4f30b: Preparing\n",
      "4a36b0c85768: Preparing\n",
      "6addedefeb30: Preparing\n",
      "f21b8e6382b2: Preparing\n",
      "3be02ac4f30b: Waiting\n",
      "4a36b0c85768: Waiting\n",
      "6addedefeb30: Waiting\n",
      "f21b8e6382b2: Waiting\n",
      "a1c8c36e3146: Preparing\n",
      "a1c8c36e3146: Waiting\n",
      "88c6b56a015e: Preparing\n",
      "d4c2f7022d7b: Preparing\n",
      "88c6b56a015e: Waiting\n",
      "d4c2f7022d7b: Waiting\n",
      "37d54eb19c68: Preparing\n",
      "e7c49386baa5: Preparing\n",
      "37d54eb19c68: Waiting\n",
      "e7c49386baa5: Waiting\n",
      "6723bb391f86: Preparing\n",
      "2b77c1837fdd: Preparing\n",
      "c308ea0f2bb6: Preparing\n",
      "91e1df25c605: Preparing\n",
      "72b51d6d1d55: Preparing\n",
      "90dfc325730b: Preparing\n",
      "ef7f8b990cf9: Preparing\n",
      "454df595b059: Preparing\n",
      "078266e4cb4c: Preparing\n",
      "fb99a34a5b47: Preparing\n",
      "6723bb391f86: Waiting\n",
      "454df595b059: Waiting\n",
      "2b77c1837fdd: Waiting\n",
      "91e1df25c605: Waiting\n",
      "72b51d6d1d55: Waiting\n",
      "90dfc325730b: Waiting\n",
      "c308ea0f2bb6: Waiting\n",
      "078266e4cb4c: Waiting\n",
      "fb99a34a5b47: Waiting\n",
      "ef7f8b990cf9: Waiting\n",
      "79c3fd430d0b: Preparing\n",
      "4a1518ebc26e: Preparing\n",
      "79c3fd430d0b: Waiting\n",
      "4a1518ebc26e: Waiting\n",
      "8f3dd49b5d19: Layer already exists\n",
      "555315c92a50: Layer already exists\n",
      "1c4281c1b467: Layer already exists\n",
      "3be02ac4f30b: Layer already exists\n",
      "4a36b0c85768: Layer already exists\n",
      "6addedefeb30: Layer already exists\n",
      "f21b8e6382b2: Layer already exists\n",
      "88c6b56a015e: Layer already exists\n",
      "a1c8c36e3146: Layer already exists\n",
      "d4c2f7022d7b: Layer already exists\n",
      "37d54eb19c68: Layer already exists\n",
      "e7c49386baa5: Layer already exists\n",
      "bbe00fe64b55: Pushed\n",
      "6723bb391f86: Layer already exists\n",
      "2b77c1837fdd: Layer already exists\n",
      "c308ea0f2bb6: Layer already exists\n",
      "91e1df25c605: Layer already exists\n",
      "72b51d6d1d55: Layer already exists\n",
      "90dfc325730b: Layer already exists\n",
      "ef7f8b990cf9: Layer already exists\n",
      "454df595b059: Layer already exists\n",
      "fb99a34a5b47: Layer already exists\n",
      "078266e4cb4c: Layer already exists\n",
      "79c3fd430d0b: Layer already exists\n",
      "4a1518ebc26e: Pushed\n",
      "9501e9fcc404: Pushed\n",
      "latest: digest: sha256:684aafe8ed6d7bc5aafe47555dc3fff35d259398272fb4404afe7aee25ace543 size: 5783\n"
     ]
    }
   ],
   "source": [
    "# Re-tag images to remote ECR registry\n",
    "!docker tag {image_name}:latest public.ecr.aws/q1p8o7w7/{image_name}:latest\n",
    "# Push image to remote AWS ECR\n",
    "!docker push public.ecr.aws/q1p8o7w7/{image_name}:latest"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
