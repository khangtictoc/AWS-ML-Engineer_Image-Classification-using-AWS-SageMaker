{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "NUM_OUTPUT_LABELS = 133\n",
    "OUTPUT_MODEL_PATH = \"./model/resnet.pth\"\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    '''\n",
    "        Complete this function that can take a model and a \n",
    "        testing data loader and will get the test accuray/loss of the model\n",
    "        Remember to include any debugging/profiling hooks that you might need\n",
    "    '''\n",
    "    print(\"##########################################\")\n",
    "    print(\"# Testing Model on Whole Testing Dataset #\")\n",
    "    print(\"##########################################\")\n",
    "\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels).item()\n",
    "\n",
    "    total_loss = running_loss / len(test_loader.dataset)\n",
    "    total_acc = running_corrects / len(test_loader.dataset)\n",
    "    print(f\"Testing Accuracy: {100 * total_acc}, Testing Loss: {total_loss}\")\n",
    "\n",
    "\n",
    "def train(model, train_loader, validation_loader, criterion, optimizer, device):\n",
    "    '''\n",
    "        Complete this function that can take a model and\n",
    "        data loaders for training and will get train the model\n",
    "        Remember to include any debugging/profiling hooks that you might need\n",
    "    '''\n",
    "\n",
    "    print(\"##################################################\")\n",
    "    print(\"# Training Model on train and validation Dataset #\")\n",
    "    print(\"##################################################\")\n",
    "\n",
    "    epochs = 2\n",
    "    best_loss = 1e6\n",
    "    loss_counter = 0\n",
    "    image_dataset={'train': train_loader, 'valid': validation_loader}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for phase in ['train', 'valid']:\n",
    "            print(f\"Epoch {epoch}, Phase {phase}\")\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_samples=0\n",
    "\n",
    "            for step, (inputs, labels) in enumerate(image_dataset[phase]):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                _,  preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels).item()\n",
    "                running_samples += len(inputs)\n",
    "\n",
    "                # Print training status (accuracy and loss)\n",
    "                accuracy = running_corrects / running_samples\n",
    "                print(\"Images [{}/{} ({:.0f}%)] Loss: {:.2f} Accuracy: {}/{} ({:.2f}%) Time: {}\".format(\n",
    "                        running_samples,\n",
    "                        len(image_dataset[phase].dataset),\n",
    "                        100.0 * (running_samples / len(image_dataset[phase].dataset)),\n",
    "                        loss.item(),\n",
    "                        running_corrects,\n",
    "                        running_samples,\n",
    "                        100.0*accuracy,\n",
    "                        time.asctime() # for measuring time for testing, remove for students and in the formatting\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                #NOTE: Comment lines below to train and test on whole dataset\n",
    "                if running_samples > (0.2 * len(image_dataset[phase].dataset)):\n",
    "                    break\n",
    "            \n",
    "            epoch_loss = running_loss / running_samples\n",
    "            epoch_acc = running_corrects / running_samples\n",
    "\n",
    "            if phase == 'valid':\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                else:\n",
    "                    loss_counter += 1\n",
    "        \n",
    "        if loss_counter == 1:\n",
    "            break\n",
    "\n",
    "    return model\n",
    "\n",
    "def net():\n",
    "    '''\n",
    "        Complete this function that initializes your model\n",
    "        Remember to use a pretrained model\n",
    "    '''\n",
    "    model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    # Freeze Model Parameters -> No need to update the pretrained parameters\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(nn.Linear(num_features, NUM_OUTPUT_LABELS))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_data_loaders(data_path, batch_size):\n",
    "    '''\n",
    "    This is an optional function that you may or may not need to implement\n",
    "    depending on whether you need to use data loaders or not\n",
    "    '''\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size=(224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.ImageFolder(data_path, transform=transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "def save_model(model, output_path):\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    torch.save(model, output_path)\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch Resnet Training \")\n",
    "    parser.add_argument(\n",
    "        \"--batch-size\",\n",
    "        type=int,\n",
    "        default=64,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for training (default: 64)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test-batch-size\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for testing (default: 1000)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation-batch-size\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        metavar=\"N\",\n",
    "        help=\"input batch size for validation (default: 1000)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=14,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 14)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr\", type=float, default=1.0, metavar=\"LR\", help=\"learning rate (default: 1.0)\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Device cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create data loaders\n",
    "train_loader = create_data_loaders(\"./dataset/dogImages/train\", 16)\n",
    "validation_loader = create_data_loaders(\"./dataset/dogImages/valid\", 16)\n",
    "test_loader = create_data_loaders(\"./dataset/dogImages/test\", 16)\n",
    "\n",
    "# Initialize a model by calling the net function\n",
    "model=net()\n",
    "\n",
    "# Create your loss and optimizer\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "'''\n",
    "Call the train function to start training your model\n",
    "Remember that you will need to set up a way to get training data from S3\n",
    "'''\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on Device {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "# Training Model on train and validation Dataset #\n",
      "##################################################\n",
      "Epoch 0, Phase train\n",
      "Images [16/6680 (0%)] Loss: 4.98 Accuracy: 0/16 (0.00%) Time: Mon May 20 09:05:33 2024\n",
      "Images [32/6680 (0%)] Loss: 5.21 Accuracy: 0/32 (0.00%) Time: Mon May 20 09:05:34 2024\n",
      "Images [48/6680 (1%)] Loss: 5.63 Accuracy: 0/48 (0.00%) Time: Mon May 20 09:05:36 2024\n",
      "Images [64/6680 (1%)] Loss: 5.77 Accuracy: 0/64 (0.00%) Time: Mon May 20 09:05:37 2024\n",
      "Images [80/6680 (1%)] Loss: 5.86 Accuracy: 0/80 (0.00%) Time: Mon May 20 09:05:38 2024\n",
      "Images [96/6680 (1%)] Loss: 6.26 Accuracy: 1/96 (1.04%) Time: Mon May 20 09:05:39 2024\n",
      "Images [112/6680 (2%)] Loss: 5.60 Accuracy: 2/112 (1.79%) Time: Mon May 20 09:05:41 2024\n",
      "Images [128/6680 (2%)] Loss: 7.10 Accuracy: 2/128 (1.56%) Time: Mon May 20 09:05:42 2024\n",
      "Images [144/6680 (2%)] Loss: 6.64 Accuracy: 3/144 (2.08%) Time: Mon May 20 09:05:43 2024\n",
      "Images [160/6680 (2%)] Loss: 6.95 Accuracy: 3/160 (1.88%) Time: Mon May 20 09:05:44 2024\n",
      "Images [176/6680 (3%)] Loss: 4.21 Accuracy: 5/176 (2.84%) Time: Mon May 20 09:05:45 2024\n",
      "Images [192/6680 (3%)] Loss: 6.78 Accuracy: 6/192 (3.12%) Time: Mon May 20 09:05:47 2024\n",
      "Images [208/6680 (3%)] Loss: 5.09 Accuracy: 7/208 (3.37%) Time: Mon May 20 09:05:48 2024\n",
      "Images [224/6680 (3%)] Loss: 6.39 Accuracy: 7/224 (3.12%) Time: Mon May 20 09:05:49 2024\n",
      "Images [240/6680 (4%)] Loss: 5.17 Accuracy: 7/240 (2.92%) Time: Mon May 20 09:05:51 2024\n",
      "Images [256/6680 (4%)] Loss: 6.27 Accuracy: 9/256 (3.52%) Time: Mon May 20 09:05:52 2024\n",
      "Images [272/6680 (4%)] Loss: 5.23 Accuracy: 11/272 (4.04%) Time: Mon May 20 09:05:53 2024\n",
      "Images [288/6680 (4%)] Loss: 5.23 Accuracy: 12/288 (4.17%) Time: Mon May 20 09:05:55 2024\n",
      "Images [304/6680 (5%)] Loss: 5.27 Accuracy: 14/304 (4.61%) Time: Mon May 20 09:05:56 2024\n",
      "Images [320/6680 (5%)] Loss: 5.57 Accuracy: 15/320 (4.69%) Time: Mon May 20 09:05:57 2024\n",
      "Images [336/6680 (5%)] Loss: 6.21 Accuracy: 16/336 (4.76%) Time: Mon May 20 09:05:59 2024\n",
      "Images [352/6680 (5%)] Loss: 5.52 Accuracy: 19/352 (5.40%) Time: Mon May 20 09:06:00 2024\n",
      "Images [368/6680 (6%)] Loss: 5.96 Accuracy: 21/368 (5.71%) Time: Mon May 20 09:06:01 2024\n",
      "Images [384/6680 (6%)] Loss: 5.03 Accuracy: 23/384 (5.99%) Time: Mon May 20 09:06:02 2024\n",
      "Images [400/6680 (6%)] Loss: 5.83 Accuracy: 23/400 (5.75%) Time: Mon May 20 09:06:04 2024\n",
      "Images [416/6680 (6%)] Loss: 4.32 Accuracy: 27/416 (6.49%) Time: Mon May 20 09:06:05 2024\n",
      "Images [432/6680 (6%)] Loss: 4.20 Accuracy: 30/432 (6.94%) Time: Mon May 20 09:06:06 2024\n",
      "Images [448/6680 (7%)] Loss: 4.60 Accuracy: 32/448 (7.14%) Time: Mon May 20 09:06:07 2024\n",
      "Images [464/6680 (7%)] Loss: 4.89 Accuracy: 32/464 (6.90%) Time: Mon May 20 09:06:09 2024\n",
      "Images [480/6680 (7%)] Loss: 4.51 Accuracy: 33/480 (6.88%) Time: Mon May 20 09:06:10 2024\n",
      "Images [496/6680 (7%)] Loss: 4.57 Accuracy: 34/496 (6.85%) Time: Mon May 20 09:06:12 2024\n",
      "Images [512/6680 (8%)] Loss: 4.26 Accuracy: 37/512 (7.23%) Time: Mon May 20 09:06:13 2024\n",
      "Images [528/6680 (8%)] Loss: 4.74 Accuracy: 37/528 (7.01%) Time: Mon May 20 09:06:15 2024\n",
      "Images [544/6680 (8%)] Loss: 4.30 Accuracy: 39/544 (7.17%) Time: Mon May 20 09:06:16 2024\n",
      "Images [560/6680 (8%)] Loss: 4.26 Accuracy: 42/560 (7.50%) Time: Mon May 20 09:06:17 2024\n",
      "Images [576/6680 (9%)] Loss: 3.62 Accuracy: 47/576 (8.16%) Time: Mon May 20 09:06:19 2024\n",
      "Images [592/6680 (9%)] Loss: 3.85 Accuracy: 50/592 (8.45%) Time: Mon May 20 09:06:20 2024\n",
      "Images [608/6680 (9%)] Loss: 4.11 Accuracy: 53/608 (8.72%) Time: Mon May 20 09:06:22 2024\n",
      "Images [624/6680 (9%)] Loss: 4.00 Accuracy: 57/624 (9.13%) Time: Mon May 20 09:06:24 2024\n",
      "Images [640/6680 (10%)] Loss: 3.87 Accuracy: 63/640 (9.84%) Time: Mon May 20 09:06:27 2024\n",
      "Images [656/6680 (10%)] Loss: 3.51 Accuracy: 66/656 (10.06%) Time: Mon May 20 09:06:28 2024\n",
      "Images [672/6680 (10%)] Loss: 4.04 Accuracy: 72/672 (10.71%) Time: Mon May 20 09:06:30 2024\n",
      "Images [688/6680 (10%)] Loss: 3.94 Accuracy: 74/688 (10.76%) Time: Mon May 20 09:06:31 2024\n",
      "Images [704/6680 (11%)] Loss: 4.12 Accuracy: 77/704 (10.94%) Time: Mon May 20 09:06:33 2024\n",
      "Images [720/6680 (11%)] Loss: 3.52 Accuracy: 81/720 (11.25%) Time: Mon May 20 09:06:35 2024\n",
      "Images [736/6680 (11%)] Loss: 3.17 Accuracy: 87/736 (11.82%) Time: Mon May 20 09:06:36 2024\n",
      "Images [752/6680 (11%)] Loss: 3.73 Accuracy: 90/752 (11.97%) Time: Mon May 20 09:06:39 2024\n",
      "Images [768/6680 (11%)] Loss: 4.16 Accuracy: 92/768 (11.98%) Time: Mon May 20 09:06:40 2024\n",
      "Images [784/6680 (12%)] Loss: 4.15 Accuracy: 97/784 (12.37%) Time: Mon May 20 09:06:42 2024\n",
      "Images [800/6680 (12%)] Loss: 3.77 Accuracy: 102/800 (12.75%) Time: Mon May 20 09:06:44 2024\n",
      "Images [816/6680 (12%)] Loss: 2.90 Accuracy: 110/816 (13.48%) Time: Mon May 20 09:06:46 2024\n",
      "Images [832/6680 (12%)] Loss: 3.61 Accuracy: 113/832 (13.58%) Time: Mon May 20 09:06:48 2024\n",
      "Images [848/6680 (13%)] Loss: 3.14 Accuracy: 119/848 (14.03%) Time: Mon May 20 09:06:50 2024\n",
      "Images [864/6680 (13%)] Loss: 3.59 Accuracy: 125/864 (14.47%) Time: Mon May 20 09:06:52 2024\n",
      "Images [880/6680 (13%)] Loss: 3.94 Accuracy: 130/880 (14.77%) Time: Mon May 20 09:06:53 2024\n",
      "Images [896/6680 (13%)] Loss: 3.70 Accuracy: 133/896 (14.84%) Time: Mon May 20 09:06:55 2024\n",
      "Images [912/6680 (14%)] Loss: 3.82 Accuracy: 139/912 (15.24%) Time: Mon May 20 09:06:57 2024\n",
      "Images [928/6680 (14%)] Loss: 3.35 Accuracy: 144/928 (15.52%) Time: Mon May 20 09:06:59 2024\n",
      "Images [944/6680 (14%)] Loss: 3.19 Accuracy: 147/944 (15.57%) Time: Mon May 20 09:07:00 2024\n",
      "Images [960/6680 (14%)] Loss: 2.72 Accuracy: 153/960 (15.94%) Time: Mon May 20 09:07:02 2024\n",
      "Images [976/6680 (15%)] Loss: 3.29 Accuracy: 157/976 (16.09%) Time: Mon May 20 09:07:04 2024\n",
      "Images [992/6680 (15%)] Loss: 3.24 Accuracy: 162/992 (16.33%) Time: Mon May 20 09:07:05 2024\n",
      "Images [1008/6680 (15%)] Loss: 3.67 Accuracy: 166/1008 (16.47%) Time: Mon May 20 09:07:07 2024\n",
      "Images [1024/6680 (15%)] Loss: 3.42 Accuracy: 169/1024 (16.50%) Time: Mon May 20 09:07:08 2024\n",
      "Images [1040/6680 (16%)] Loss: 3.62 Accuracy: 175/1040 (16.83%) Time: Mon May 20 09:07:10 2024\n",
      "Images [1056/6680 (16%)] Loss: 2.67 Accuracy: 180/1056 (17.05%) Time: Mon May 20 09:07:12 2024\n",
      "Images [1072/6680 (16%)] Loss: 3.30 Accuracy: 185/1072 (17.26%) Time: Mon May 20 09:07:13 2024\n",
      "Images [1088/6680 (16%)] Loss: 2.72 Accuracy: 192/1088 (17.65%) Time: Mon May 20 09:07:16 2024\n",
      "Images [1104/6680 (17%)] Loss: 3.19 Accuracy: 197/1104 (17.84%) Time: Mon May 20 09:07:18 2024\n",
      "Images [1120/6680 (17%)] Loss: 3.09 Accuracy: 203/1120 (18.12%) Time: Mon May 20 09:07:19 2024\n",
      "Images [1136/6680 (17%)] Loss: 2.85 Accuracy: 209/1136 (18.40%) Time: Mon May 20 09:07:21 2024\n",
      "Images [1152/6680 (17%)] Loss: 2.48 Accuracy: 217/1152 (18.84%) Time: Mon May 20 09:07:22 2024\n",
      "Images [1168/6680 (17%)] Loss: 3.38 Accuracy: 221/1168 (18.92%) Time: Mon May 20 09:07:24 2024\n",
      "Images [1184/6680 (18%)] Loss: 2.99 Accuracy: 226/1184 (19.09%) Time: Mon May 20 09:07:25 2024\n",
      "Images [1200/6680 (18%)] Loss: 3.13 Accuracy: 233/1200 (19.42%) Time: Mon May 20 09:07:27 2024\n",
      "Images [1216/6680 (18%)] Loss: 2.90 Accuracy: 237/1216 (19.49%) Time: Mon May 20 09:07:29 2024\n",
      "Images [1232/6680 (18%)] Loss: 3.13 Accuracy: 241/1232 (19.56%) Time: Mon May 20 09:07:30 2024\n",
      "Images [1248/6680 (19%)] Loss: 2.78 Accuracy: 247/1248 (19.79%) Time: Mon May 20 09:07:31 2024\n",
      "Images [1264/6680 (19%)] Loss: 2.39 Accuracy: 256/1264 (20.25%) Time: Mon May 20 09:07:33 2024\n",
      "Images [1280/6680 (19%)] Loss: 3.14 Accuracy: 261/1280 (20.39%) Time: Mon May 20 09:07:34 2024\n",
      "Images [1296/6680 (19%)] Loss: 2.93 Accuracy: 266/1296 (20.52%) Time: Mon May 20 09:07:36 2024\n",
      "Images [1312/6680 (20%)] Loss: 2.42 Accuracy: 271/1312 (20.66%) Time: Mon May 20 09:07:37 2024\n",
      "Images [1328/6680 (20%)] Loss: 2.52 Accuracy: 278/1328 (20.93%) Time: Mon May 20 09:07:39 2024\n",
      "Images [1344/6680 (20%)] Loss: 3.04 Accuracy: 283/1344 (21.06%) Time: Mon May 20 09:07:40 2024\n",
      "Epoch 0, Phase valid\n",
      "Images [16/835 (2%)] Loss: 2.61 Accuracy: 5/16 (31.25%) Time: Mon May 20 09:07:42 2024\n",
      "Images [32/835 (4%)] Loss: 2.57 Accuracy: 11/32 (34.38%) Time: Mon May 20 09:07:43 2024\n",
      "Images [48/835 (6%)] Loss: 2.70 Accuracy: 15/48 (31.25%) Time: Mon May 20 09:07:44 2024\n",
      "Images [64/835 (8%)] Loss: 2.56 Accuracy: 21/64 (32.81%) Time: Mon May 20 09:07:46 2024\n",
      "Images [80/835 (10%)] Loss: 2.84 Accuracy: 27/80 (33.75%) Time: Mon May 20 09:07:47 2024\n",
      "Images [96/835 (11%)] Loss: 2.75 Accuracy: 31/96 (32.29%) Time: Mon May 20 09:07:49 2024\n",
      "Images [112/835 (13%)] Loss: 2.29 Accuracy: 37/112 (33.04%) Time: Mon May 20 09:07:50 2024\n",
      "Images [128/835 (15%)] Loss: 2.38 Accuracy: 44/128 (34.38%) Time: Mon May 20 09:07:52 2024\n",
      "Images [144/835 (17%)] Loss: 2.00 Accuracy: 52/144 (36.11%) Time: Mon May 20 09:07:54 2024\n",
      "Images [160/835 (19%)] Loss: 2.78 Accuracy: 58/160 (36.25%) Time: Mon May 20 09:07:55 2024\n",
      "Images [176/835 (21%)] Loss: 2.31 Accuracy: 65/176 (36.93%) Time: Mon May 20 09:07:56 2024\n",
      "Epoch 1, Phase train\n",
      "Images [16/6680 (0%)] Loss: 2.73 Accuracy: 5/16 (31.25%) Time: Mon May 20 09:07:58 2024\n",
      "Images [32/6680 (0%)] Loss: 2.27 Accuracy: 13/32 (40.62%) Time: Mon May 20 09:08:00 2024\n",
      "Images [48/6680 (1%)] Loss: 2.39 Accuracy: 20/48 (41.67%) Time: Mon May 20 09:08:01 2024\n",
      "Images [64/6680 (1%)] Loss: 2.06 Accuracy: 29/64 (45.31%) Time: Mon May 20 09:08:03 2024\n",
      "Images [80/6680 (1%)] Loss: 3.13 Accuracy: 32/80 (40.00%) Time: Mon May 20 09:08:05 2024\n",
      "Images [96/6680 (1%)] Loss: 2.64 Accuracy: 40/96 (41.67%) Time: Mon May 20 09:08:06 2024\n",
      "Images [112/6680 (2%)] Loss: 2.48 Accuracy: 45/112 (40.18%) Time: Mon May 20 09:08:08 2024\n",
      "Images [128/6680 (2%)] Loss: 2.22 Accuracy: 55/128 (42.97%) Time: Mon May 20 09:08:10 2024\n",
      "Images [144/6680 (2%)] Loss: 2.61 Accuracy: 62/144 (43.06%) Time: Mon May 20 09:08:11 2024\n",
      "Images [160/6680 (2%)] Loss: 2.52 Accuracy: 69/160 (43.12%) Time: Mon May 20 09:08:13 2024\n",
      "Images [176/6680 (3%)] Loss: 2.40 Accuracy: 78/176 (44.32%) Time: Mon May 20 09:08:15 2024\n",
      "Images [192/6680 (3%)] Loss: 2.49 Accuracy: 83/192 (43.23%) Time: Mon May 20 09:08:16 2024\n",
      "Images [208/6680 (3%)] Loss: 2.32 Accuracy: 91/208 (43.75%) Time: Mon May 20 09:08:18 2024\n",
      "Images [224/6680 (3%)] Loss: 2.39 Accuracy: 98/224 (43.75%) Time: Mon May 20 09:08:19 2024\n",
      "Images [240/6680 (4%)] Loss: 1.79 Accuracy: 108/240 (45.00%) Time: Mon May 20 09:08:21 2024\n",
      "Images [256/6680 (4%)] Loss: 3.18 Accuracy: 113/256 (44.14%) Time: Mon May 20 09:08:23 2024\n",
      "Images [272/6680 (4%)] Loss: 2.20 Accuracy: 120/272 (44.12%) Time: Mon May 20 09:08:24 2024\n",
      "Images [288/6680 (4%)] Loss: 2.35 Accuracy: 127/288 (44.10%) Time: Mon May 20 09:08:26 2024\n",
      "Images [304/6680 (5%)] Loss: 1.81 Accuracy: 138/304 (45.39%) Time: Mon May 20 09:08:27 2024\n",
      "Images [320/6680 (5%)] Loss: 1.65 Accuracy: 148/320 (46.25%) Time: Mon May 20 09:08:29 2024\n",
      "Images [336/6680 (5%)] Loss: 1.99 Accuracy: 156/336 (46.43%) Time: Mon May 20 09:08:31 2024\n",
      "Images [352/6680 (5%)] Loss: 2.24 Accuracy: 166/352 (47.16%) Time: Mon May 20 09:08:32 2024\n",
      "Images [368/6680 (6%)] Loss: 1.75 Accuracy: 177/368 (48.10%) Time: Mon May 20 09:08:34 2024\n",
      "Images [384/6680 (6%)] Loss: 1.92 Accuracy: 186/384 (48.44%) Time: Mon May 20 09:08:36 2024\n",
      "Images [400/6680 (6%)] Loss: 1.98 Accuracy: 194/400 (48.50%) Time: Mon May 20 09:08:38 2024\n",
      "Images [416/6680 (6%)] Loss: 2.17 Accuracy: 203/416 (48.80%) Time: Mon May 20 09:08:39 2024\n",
      "Images [432/6680 (6%)] Loss: 1.93 Accuracy: 213/432 (49.31%) Time: Mon May 20 09:08:41 2024\n",
      "Images [448/6680 (7%)] Loss: 2.26 Accuracy: 221/448 (49.33%) Time: Mon May 20 09:08:43 2024\n",
      "Images [464/6680 (7%)] Loss: 2.55 Accuracy: 227/464 (48.92%) Time: Mon May 20 09:08:44 2024\n",
      "Images [480/6680 (7%)] Loss: 1.71 Accuracy: 238/480 (49.58%) Time: Mon May 20 09:08:46 2024\n",
      "Images [496/6680 (7%)] Loss: 2.33 Accuracy: 244/496 (49.19%) Time: Mon May 20 09:08:48 2024\n",
      "Images [512/6680 (8%)] Loss: 1.82 Accuracy: 252/512 (49.22%) Time: Mon May 20 09:08:49 2024\n",
      "Images [528/6680 (8%)] Loss: 2.13 Accuracy: 258/528 (48.86%) Time: Mon May 20 09:08:51 2024\n",
      "Images [544/6680 (8%)] Loss: 1.93 Accuracy: 267/544 (49.08%) Time: Mon May 20 09:08:52 2024\n",
      "Images [560/6680 (8%)] Loss: 1.64 Accuracy: 278/560 (49.64%) Time: Mon May 20 09:08:54 2024\n",
      "Images [576/6680 (9%)] Loss: 1.24 Accuracy: 290/576 (50.35%) Time: Mon May 20 09:08:56 2024\n",
      "Images [592/6680 (9%)] Loss: 1.31 Accuracy: 302/592 (51.01%) Time: Mon May 20 09:08:58 2024\n",
      "Images [608/6680 (9%)] Loss: 1.64 Accuracy: 314/608 (51.64%) Time: Mon May 20 09:08:59 2024\n",
      "Images [624/6680 (9%)] Loss: 2.45 Accuracy: 321/624 (51.44%) Time: Mon May 20 09:09:01 2024\n",
      "Images [640/6680 (10%)] Loss: 1.53 Accuracy: 331/640 (51.72%) Time: Mon May 20 09:09:03 2024\n",
      "Images [656/6680 (10%)] Loss: 1.71 Accuracy: 342/656 (52.13%) Time: Mon May 20 09:09:05 2024\n",
      "Images [672/6680 (10%)] Loss: 2.51 Accuracy: 348/672 (51.79%) Time: Mon May 20 09:09:07 2024\n",
      "Images [688/6680 (10%)] Loss: 1.97 Accuracy: 355/688 (51.60%) Time: Mon May 20 09:09:08 2024\n",
      "Images [704/6680 (11%)] Loss: 1.38 Accuracy: 367/704 (52.13%) Time: Mon May 20 09:09:10 2024\n",
      "Images [720/6680 (11%)] Loss: 1.41 Accuracy: 379/720 (52.64%) Time: Mon May 20 09:09:12 2024\n",
      "Images [736/6680 (11%)] Loss: 1.62 Accuracy: 388/736 (52.72%) Time: Mon May 20 09:09:13 2024\n",
      "Images [752/6680 (11%)] Loss: 2.22 Accuracy: 396/752 (52.66%) Time: Mon May 20 09:09:15 2024\n",
      "Images [768/6680 (11%)] Loss: 2.68 Accuracy: 402/768 (52.34%) Time: Mon May 20 09:09:16 2024\n",
      "Images [784/6680 (12%)] Loss: 2.14 Accuracy: 409/784 (52.17%) Time: Mon May 20 09:09:18 2024\n",
      "Images [800/6680 (12%)] Loss: 1.90 Accuracy: 418/800 (52.25%) Time: Mon May 20 09:09:19 2024\n",
      "Images [816/6680 (12%)] Loss: 1.63 Accuracy: 428/816 (52.45%) Time: Mon May 20 09:09:21 2024\n",
      "Images [832/6680 (12%)] Loss: 2.05 Accuracy: 435/832 (52.28%) Time: Mon May 20 09:09:22 2024\n",
      "Images [848/6680 (13%)] Loss: 2.29 Accuracy: 441/848 (52.00%) Time: Mon May 20 09:09:24 2024\n",
      "Images [864/6680 (13%)] Loss: 2.19 Accuracy: 449/864 (51.97%) Time: Mon May 20 09:09:25 2024\n",
      "Images [880/6680 (13%)] Loss: 1.74 Accuracy: 458/880 (52.05%) Time: Mon May 20 09:09:27 2024\n",
      "Images [896/6680 (13%)] Loss: 1.73 Accuracy: 466/896 (52.01%) Time: Mon May 20 09:09:29 2024\n",
      "Images [912/6680 (14%)] Loss: 1.24 Accuracy: 478/912 (52.41%) Time: Mon May 20 09:09:30 2024\n",
      "Images [928/6680 (14%)] Loss: 1.23 Accuracy: 489/928 (52.69%) Time: Mon May 20 09:09:32 2024\n",
      "Images [944/6680 (14%)] Loss: 2.11 Accuracy: 496/944 (52.54%) Time: Mon May 20 09:09:34 2024\n",
      "Images [960/6680 (14%)] Loss: 1.45 Accuracy: 507/960 (52.81%) Time: Mon May 20 09:09:36 2024\n",
      "Images [976/6680 (15%)] Loss: 2.64 Accuracy: 513/976 (52.56%) Time: Mon May 20 09:09:38 2024\n",
      "Images [992/6680 (15%)] Loss: 1.89 Accuracy: 522/992 (52.62%) Time: Mon May 20 09:09:40 2024\n",
      "Images [1008/6680 (15%)] Loss: 2.21 Accuracy: 529/1008 (52.48%) Time: Mon May 20 09:09:42 2024\n",
      "Images [1024/6680 (15%)] Loss: 1.12 Accuracy: 542/1024 (52.93%) Time: Mon May 20 09:09:44 2024\n",
      "Images [1040/6680 (16%)] Loss: 1.45 Accuracy: 553/1040 (53.17%) Time: Mon May 20 09:09:45 2024\n",
      "Images [1056/6680 (16%)] Loss: 1.13 Accuracy: 567/1056 (53.69%) Time: Mon May 20 09:09:47 2024\n",
      "Images [1072/6680 (16%)] Loss: 2.00 Accuracy: 577/1072 (53.82%) Time: Mon May 20 09:09:49 2024\n",
      "Images [1088/6680 (16%)] Loss: 1.42 Accuracy: 586/1088 (53.86%) Time: Mon May 20 09:09:50 2024\n",
      "Images [1104/6680 (17%)] Loss: 2.18 Accuracy: 592/1104 (53.62%) Time: Mon May 20 09:09:52 2024\n",
      "Images [1120/6680 (17%)] Loss: 1.70 Accuracy: 602/1120 (53.75%) Time: Mon May 20 09:09:53 2024\n",
      "Images [1136/6680 (17%)] Loss: 1.63 Accuracy: 610/1136 (53.70%) Time: Mon May 20 09:09:55 2024\n",
      "Images [1152/6680 (17%)] Loss: 1.62 Accuracy: 620/1152 (53.82%) Time: Mon May 20 09:09:57 2024\n",
      "Images [1168/6680 (17%)] Loss: 1.23 Accuracy: 630/1168 (53.94%) Time: Mon May 20 09:09:58 2024\n",
      "Images [1184/6680 (18%)] Loss: 2.10 Accuracy: 636/1184 (53.72%) Time: Mon May 20 09:10:00 2024\n",
      "Images [1200/6680 (18%)] Loss: 1.35 Accuracy: 646/1200 (53.83%) Time: Mon May 20 09:10:01 2024\n",
      "Images [1216/6680 (18%)] Loss: 1.71 Accuracy: 655/1216 (53.87%) Time: Mon May 20 09:10:03 2024\n",
      "Images [1232/6680 (18%)] Loss: 1.45 Accuracy: 667/1232 (54.14%) Time: Mon May 20 09:10:04 2024\n",
      "Images [1248/6680 (19%)] Loss: 1.69 Accuracy: 677/1248 (54.25%) Time: Mon May 20 09:10:06 2024\n",
      "Images [1264/6680 (19%)] Loss: 1.25 Accuracy: 688/1264 (54.43%) Time: Mon May 20 09:10:08 2024\n",
      "Images [1280/6680 (19%)] Loss: 1.66 Accuracy: 697/1280 (54.45%) Time: Mon May 20 09:10:09 2024\n",
      "Images [1296/6680 (19%)] Loss: 1.49 Accuracy: 708/1296 (54.63%) Time: Mon May 20 09:10:11 2024\n",
      "Images [1312/6680 (20%)] Loss: 1.19 Accuracy: 719/1312 (54.80%) Time: Mon May 20 09:10:12 2024\n",
      "Images [1328/6680 (20%)] Loss: 2.03 Accuracy: 726/1328 (54.67%) Time: Mon May 20 09:10:14 2024\n",
      "Images [1344/6680 (20%)] Loss: 1.63 Accuracy: 737/1344 (54.84%) Time: Mon May 20 09:10:16 2024\n",
      "Epoch 1, Phase valid\n",
      "Images [16/835 (2%)] Loss: 1.54 Accuracy: 10/16 (62.50%) Time: Mon May 20 09:10:18 2024\n",
      "Images [32/835 (4%)] Loss: 1.62 Accuracy: 21/32 (65.62%) Time: Mon May 20 09:10:19 2024\n",
      "Images [48/835 (6%)] Loss: 1.45 Accuracy: 32/48 (66.67%) Time: Mon May 20 09:10:21 2024\n",
      "Images [64/835 (8%)] Loss: 1.04 Accuracy: 43/64 (67.19%) Time: Mon May 20 09:10:22 2024\n",
      "Images [80/835 (10%)] Loss: 1.31 Accuracy: 51/80 (63.75%) Time: Mon May 20 09:10:24 2024\n",
      "Images [96/835 (11%)] Loss: 1.47 Accuracy: 62/96 (64.58%) Time: Mon May 20 09:10:25 2024\n",
      "Images [112/835 (13%)] Loss: 0.96 Accuracy: 73/112 (65.18%) Time: Mon May 20 09:10:27 2024\n",
      "Images [128/835 (15%)] Loss: 1.30 Accuracy: 82/128 (64.06%) Time: Mon May 20 09:10:28 2024\n",
      "Images [144/835 (17%)] Loss: 1.57 Accuracy: 92/144 (63.89%) Time: Mon May 20 09:10:30 2024\n",
      "Images [160/835 (19%)] Loss: 1.57 Accuracy: 102/160 (63.75%) Time: Mon May 20 09:10:31 2024\n",
      "Images [176/835 (21%)] Loss: 1.43 Accuracy: 113/176 (64.20%) Time: Mon May 20 09:10:33 2024\n"
     ]
    }
   ],
   "source": [
    "model = train(model, train_loader, validation_loader, loss_criterion, optimizer, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "File ./model/resnet50.pth cannot be opened.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_MODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 162\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(model, output_path)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_path):\n\u001b[0;32m    161\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_path)\n\u001b[1;32m--> 162\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: File ./model/resnet50.pth cannot be opened."
     ]
    }
   ],
   "source": [
    "save_model(model, OUTPUT_MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
